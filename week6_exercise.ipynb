{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sensitive-astronomy",
   "metadata": {},
   "source": [
    "> *Exercises*: A few questions about machine learning.\n",
    "\n",
    "> * What do we mean by a 'feature' in a machine learning model?\n",
    "> * What is the main problem with overfitting?\n",
    "> * Explain the connection between the bias-variance trade-off and overfitting/underfitting.\n",
    "> * The `Luke is for leukemia` on page 145 in the reading is a great example of why accuracy is not a good measure in very unbalanced problems. You know about the incidents dataset we've been working with. Try to come up with a similar example based on the data we've been working with today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-egypt",
   "metadata": {},
   "source": [
    "## Scikit-learn\n",
    "Text: [this tutorial](https://scikit-learn.org/stable/tutorial/basic/tutorial.html), sections (*Machine learning: the problem setting*, *Loading an example dataset*, *Learning and predicting*)\n",
    "> *Exercise*: Did you read the text?\n",
    ">\n",
    "> * Describe in your own words how data is organized in `sklearn` (how does a *dataset* work according to the tutorial)?\n",
    "> * What is the dimensionality of the `.data` part of a dataset and what is the size of each dimension?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-pierre",
   "metadata": {},
   "source": [
    "### Tutorial\n",
    "Now we're going to work through a [tutorial on text analytics](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n",
    "\n",
    "> *Exercise*: Did you do the work?\n",
    ">\n",
    "> * Describe in your own words the dataset used in the tutorial. \n",
    "> * Investigate further: what kind of folder/file structure does the `sklearn.datasets.load_files` function expect?\n",
    "> * What is the \"bag-of-words\" representation of text? How does this strategy turn text into data of the kind described above?\n",
    "> * (Don't worry too much about tokenization and TF-IDF for now, but do check out those part if you want to use real text analysis later)\n",
    "> * Once you've built the classifier, play around with it a bit. Describe the content of the `predicted` variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-sustainability",
   "metadata": {},
   "source": [
    "# KNN\n",
    "> *Warm up exercises*: K-nearest-neighbors\n",
    "> \n",
    "> How does K-nearest-neighbors work? Explain in your own words.\n",
    "Explain in your own words: What is the curse of dimensionality? Use figure 12-6 in DSFS as part of your explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-shepherd",
   "metadata": {},
   "source": [
    "Finally, we can start working with the crime data. Here's a little exercise\n",
    "\n",
    "\n",
    "> *Exercise*: K-nearest-neighbors map.\n",
    ">\n",
    "> We know from last week's exercises that the focus crimes `PROSTITUTION`, `DRUG/NARCOTIC` and `DRIVING UNDER THE INFLUENCE` tend to be concentrated in certain neighborhoods, so we focus on those crime types since they will make the most sense a KNN - map.\n",
    "> \n",
    "> * Begin by using `folium` (see Week4) to plot all incidents of the three crime types on their own map. This will give you an idea of how the varioius crimes are distributed across the city.\n",
    "> * Next, it's time to set up your model based on the actual data. I recommend that you try out `sklearn`'s `KNeighborsClassifier`. For an intro, start with [this tutorial](https://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html) and follow the link to get a sense of the usage.\n",
    ">   * You don't have to think a lot about testing/trainig and accuracy for this exercise. We're mostly interested in creating a map that's not too problematic. But do calculate the number of observations of each crime-type respectively. You'll find that the levels of each crime varies (lots of drug arrests, an intermediate amount of prostitiution registered, and very little drunk driving in the dataset). Since the algorithm classifies each point according to it's neighbors, *what could a consequence of this imbalance in the number of examples from each class mean for your map*?\n",
    ">   * You can make the dataset 'balanced' by grabbing an equal number of examples from each crime category. \n",
    ">       * How do you expect that will change the KNN result? \n",
    ">       * In which situations is the balanced map useful - \n",
    ">       * When is the map where data is in proportion to occurrences useful? \n",
    ">       * Choose which map you will work on in the following.\n",
    "> * Now create an approximately square grid of point that runs over SF. You get to decide the grid-size, but I recommend somewhere between $50\\times50$ and $100\\times100$ points. I recommend using `folium` for this task.\n",
    "> * Visualize your model by coloring the grid, coloring each grid point according to it's category. Create a plot of this kind for models where each point is colored according to the majority of its $5$, $10$, and $30$ nearest neighbors. Describe what happens to the map as you increase the number of neighbors, `K`. \n",
    "> * To see an example, [click here](https://raw.githubusercontent.com/suneman/socialdataanalysis2020/master/files/KNN-example.png). This one is a 100x100 grid based on crimes from 1st January 2017 until the end of 2018. And the categories are narcotics, prostitution and vehicle theft."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-drill",
   "metadata": {},
   "source": [
    "## Part 4: Decision Trees"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
