{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "personalized-casting",
   "metadata": {},
   "source": [
    "# Part 1: Questions to text and lectures.\n",
    "\n",
    "## A) Segal and Heer paper\n",
    "\n",
    "### What is the *Oxford English Dictionary's* defintion of a narrative?\n",
    "\n",
    "The Oxford English Dictionary defines narrative as “an account of a series of events, facts, etc., given in order and with the establishing of connections between them.”\n",
    "\n",
    "### What is your favorite visualization among the examples in section 3? Explain why in a few words.\n",
    "\n",
    "My favorite visualization from section 3, is “Gapminder Human Development Trends”. The data is presented in a clear way and does not become unmanageable or confusing. The visualizations cover different sections of the presentation and are split into multiple screens, which can all accessed by using the progress bar. For each section, the user is guided through the visualization and important key observations are made clear by using annotations, highlighting and animated transitions. Furthermore, the user also has the freedom to stop and take a look around for themselves and inspect more specific observations by using the timeline or details-on-demand functionalities. \n",
    "\n",
    "### What's the point of Figure 7?\n",
    "\n",
    "The goal of the paper is to generalize from examples to identify salient design dimensions and in the process clarify how narrative visualization differs from other forms of storytelling. Figure 7 is made to give an overview of the most commonly used design principles and depicts the unique patterns for narrative visualizations. \n",
    "\n",
    "### Use Figure 7 to find the most common design choice within each category for the Visual narrative and Narrative structure (the categories within visual narrative are 'visual structuring', 'highlighting', etc).\n",
    "\n",
    "There are 3 categories within “Visual Narrative” namely “Visual structuring”, “Highlighting” and “Transition Guidance”, with the and most common design choices being resp. “Consistent Visual Platform”, “Feature Distinction” and “Object Continuity” as seen in the table below:\n",
    "\n",
    "| Category            | Most common choice         |\n",
    "|---------------------|----------------------------|\n",
    "| Visual structuring  | Consistent Visual Platform |\n",
    "| Highlighting        | Feature Distinction        |\n",
    "| Transition Guidance | Object Continuity          |\n",
    "\n",
    "There are also 3 categories within “Narrative Structure” namely “Ordering”, “Interactivity” and “Messaging”, with the most common design choices being resp. “User Directed Path”, “Filtering / Selection / Search” and “Captions / Headlines” as seen below:\n",
    "\n",
    "| Category      | Most common choice             |\n",
    "|---------------|--------------------------------|\n",
    "| Ordering      | User Directed Path             |\n",
    "| Interactivity | Filtering / Selection / Search |\n",
    "| Messaging     | Captions / Headlines           |\n",
    "\n",
    "### Check out Figure 8 and section 4.3. What is your favorite genre of narrative visualization? Why? What is your least favorite genre? Why?\n",
    "\n",
    "We prefer the slideshow genre, because the visualizations can be split up and displayed on different screens, all under capturing titles. It gives a clear overview of the visualizations and the sorting of them. Furthermore, it becomes easy to investigate a specific category/visualization. In the slideshow genre an order is only suggested but you are not limited to this ordering.\n",
    "\n",
    "Our least favorite genre is the magazine style, which can be categories as having author-driven design properties. Everything is displayed on a single page with heavy messaging, there are typically no interactivity and the reader is constricted to the defined linear order. This style simply relies on the author knowing what the reader wants to see, which results in a very constricted view for the reader. \n",
    "\n",
    "## B) Explanatory data visualisation\n",
    "\n",
    "### What are the three key elements to keep in mind when you design an explanatory visualization?\n",
    "\n",
    "The three key points are “Start with a question” (what do we wish to communicate through our results?), “Allow exploration” (we let the users investigate the results themselves, by implementing visualizations following D3) and “Know your readers” (we cater to the audience).\n",
    "\n",
    "* In the video I talk about (1) *overview first*,  (2) *zoom and filter*,  (3) *details on demand*.\n",
    "  - Go online and find a visualization that follows these principles (don't use one from the video).\n",
    "  - Explain how it does achieves (1)-(3). It might be useful to use screenshots to illustrate your explanation.\n",
    "\n",
    "### Example of visualisation\n",
    "The visualization found with the link https://worldpoverty.io/map, clearly demonstrate how to implement a visualization following D3. \n",
    "The overview is a present depiction of the countries dealing with poverty clearly marked on the map of the world. Both above and below the map, it is clear what factors can be used to filter with, and a timeline is made accessible to investigate previous years and forecast of the further following the SDG target.\n",
    "\n",
    "![Overview](./images/world_overview.png)\n",
    "\n",
    "The zoom functionality can be seen when clicking on a country.\n",
    "\n",
    "![Zoom](./images/world_zoom.png)\n",
    "\n",
    "The filtering functionality can be experienced by for example choosing an age interval.\n",
    "\n",
    "![Filtering](./images/world_filtering.png)\n",
    "\n",
    "The details-on-demand functionality can by seen, when holding the mouse over a country.\n",
    "\n",
    "![Details-on-demand](./images/world_demand.png)\n",
    "\n",
    "### How is explanatory data analysis different from exploratory data analysis?\n",
    "In short, an exploratory analysis is about exploring the data to find insights, while explanatory is about sharing those insights with others.\n",
    "\n",
    "To elaborate, an **exploratory** analysis gives insight in the dataset, helps us better understand it, and gives us ideas of what to investigate further. Typically, we make an exploratory analysis hoping that the visualization can confirm or disprove a hypothesis we are working from, or just point us in what directions to go in next. \n",
    "\n",
    "An **explanatory** analysis on the other hand is tailored to a specific audience. An audience we want to convey our finding to. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-marketing",
   "metadata": {},
   "source": [
    "## Part 2: Random forest and weather\n",
    "\n",
    "### A) Random forest binary classification\n",
    "\n",
    "Below is the code for Part 2A, followed by an answer to the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "valuable-fitness",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "raw_crimes = pd.read_csv(\"Police_Department_Incident_Reports__Historical_2003_to_May_2018.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "light-static",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from datetime import datetime\n",
    "\n",
    "focuscrimes = [\"VEHICLE THEFT\", \"FRAUD\"]\n",
    "\n",
    "crimes = raw_crimes[raw_crimes[\"Category\"].isin(focuscrimes)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-mobility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "cat = \"category\"\n",
    "dt = \"datetime\"\n",
    "year = \"year\"\n",
    "month = \"month\"\n",
    "day = \"day\"\n",
    "hour = \"hour\"\n",
    "hour_of_month = \"hour_of_month\"\n",
    "hour_of_week = \"hour_of_week\"\n",
    "day_of_week = \"dayofweek\"\n",
    "pddistrict = \"pddistrict\"\n",
    "\n",
    "crimes.columns = crimes.columns.str.lower()\n",
    "crimes[dt] = pd.to_datetime(crimes[\"date\"] + \" \" + crimes[\"time\"])\n",
    "\n",
    "crimes[year] = crimes[dt].dt.year\n",
    "crimes[month] = crimes[dt].dt.month\n",
    "crimes[day] = crimes[dt].dt.day\n",
    "crimes[hour] = crimes[dt].dt.hour\n",
    "\n",
    "crimes[hour_of_month] = crimes.apply(lambda row: row[dt].day * 24 + row[hour], axis=1)\n",
    "crimes[hour_of_week] = crimes.apply(lambda row: row[dt].dayofweek * 24 + row[hour], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-cutting",
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_in_range = crimes[crimes[year].between(2012, 2017, inclusive=True)]\n",
    "burglary = crimes_in_range[crimes_in_range[cat].isin([focuscrimes[0]])]\n",
    "fraud = crimes_in_range[crimes_in_range[cat].isin([focuscrimes[1]])]\n",
    "\n",
    "print(burglary.shape)\n",
    "print(fraud.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 15000\n",
    "\n",
    "# Create balanced data set\n",
    "type1 = burglary.sample(sample_size)\n",
    "type2 = fraud.sample(sample_size)\n",
    "\n",
    "crime_df = pd.concat([type1, type2], ignore_index=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_features = [cat, day_of_week, month, hour, pddistrict]\n",
    "crime_dummies = [day_of_week, pddistrict]\n",
    "\n",
    "# Let's make the crime features\n",
    "Xc = crime_df[crime_features].copy()\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "Xc[cat] = le.fit_transform(Xc[cat])\n",
    "\n",
    "# One-hot encode the categorical data\n",
    "Xc = pd.get_dummies(Xc, columns=crime_dummies)\n",
    "\n",
    "# Labels will be the values we want to predict\n",
    "yc = np.array(Xc[cat])\n",
    "\n",
    "# We remove the labels from the crime dataframe to get all the values we need for the features\n",
    "Xc = Xc.drop(cat, axis=1)\n",
    "\n",
    "# We save the feature names for later\n",
    "Xc_list = list(Xc.columns)\n",
    "\n",
    "# Convert the dataframe to a numpy array so we can work with the features\n",
    "Xc = np.array(Xc)\n",
    "\n",
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "Xc_train, Xc_test, yc_train, yc_test = train_test_split(Xc, yc, test_size = 0.25, random_state = 42)\n",
    "\n",
    "print('Training Features Shape:', Xc_train.shape)\n",
    "print('Training Labels Shape:', yc_train.shape)\n",
    "print('Testing Features Shape:', Xc_test.shape)\n",
    "print('Testing Labels Shape:', yc_test.shape)\n",
    "print('\\n')\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "cclf_max = RandomForestClassifier(n_estimators=99, random_state=42)\n",
    "cclf_max.fit(Xc_train, yc_train)\n",
    "\n",
    "print(\"Average Tree Depth:\", round(np.mean([estimator.get_depth() for estimator in cclf_max.estimators_]), 2))\n",
    "\n",
    "ctrain_pred = cclf_max.predict(Xc_train)\n",
    "print(\"Train\")\n",
    "print('- Mean Absolute Error:', round(mean_absolute_error(yc_train, ctrain_pred), 2), 'degrees.')\n",
    "print(\"- Accuracy: \", 100 * round(cclf_max.score(Xc_train, yc_train), 2), \"%\")\n",
    "\n",
    "ctest_pred = cclf_max.predict(Xc_test)\n",
    "print(\"Test\")\n",
    "print('- Mean Absolute Error:', round(mean_absolute_error(yc_test, ctest_pred), 2), 'degrees.')\n",
    "print(\"- TeAccuracy: \", 100 * round(cclf_max.score(Xc_test, yc_test), 2), \"%\\n\")\n",
    "\n",
    "\n",
    "cclf = RandomForestClassifier(n_estimators=99, random_state=42, max_depth=3)\n",
    "cclf.fit(Xc_train, yc_train)\n",
    "\n",
    "print(\"Max Depth 3\")\n",
    "\n",
    "ctrain_pred = cclf.predict(Xc_train)\n",
    "print(\"Train\")\n",
    "print('- Mean Absolute Error:', round(mean_absolute_error(yc_train, ctrain_pred), 2), 'degrees.')\n",
    "print(\"- Accuracy: \", 100 * round(cclf.score(Xc_train, yc_train), 2), \"%\")\n",
    "\n",
    "ctest_pred = cclf.predict(Xc_test)\n",
    "print(\"Test\")\n",
    "print('- Mean Absolute Error:', round(mean_absolute_error(yc_test, ctest_pred), 2), 'degrees.')\n",
    "\n",
    "# Crime accuracy\n",
    "cacc = cclf.score(Xc_test, yc_test) \n",
    "print(\"- Accuracy: \", 100 * round(cacc, 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-investigation",
   "metadata": {},
   "source": [
    "## Part 2A\n",
    "\n",
    "**Did you balance the training data? What are the pros/cons of balancing?**\n",
    "\n",
    "The dataset is balanced with 20000 randomly picked samples from each crime category, as to ensure the crime are distributed equally over time with no favor of one over the other.\n",
    "\n",
    "**Do you think your model is overfitting?**\n",
    "\n",
    "Initially where the classifier had no maximum depth, the training accuracy was near 89% where test accuracy was at 52-53%. Together with a avg. tree depth of around 45 of a dataset with 18 feautures, it seems safe to assume that the model was overfitting, as it clearly shows it did not generalize well from the training data to the testing data.\n",
    "\n",
    "However, with a maximum depth of 3, a higher accuracy is reached but with a drastical smaller tree size, which could indicate a better fitted model.\n",
    "\n",
    "**Did you choose to do cross-validation?**\n",
    "\n",
    "To error estimate the classifier, the Holdout Method is used by creating training and testing/validation datasets. The testing datasets are then used to calculate the mean accuracy of the classifier.\n",
    "\n",
    "**Which specific features did you end up using? Why?**\n",
    "\n",
    "The features used are \"DayOfWeek\", \"Date\", \"Time\", and \"PdDistrict\", because they tell something about the time and place of the crime.\n",
    "\n",
    "**Which features (if any) did you one-hot encode? Why ... or why not?))**\n",
    "\n",
    "The features to be one-hot encoded was \"DayOfWeek\" and \"PdDistrict\", where the crime category was just label encoded. Both \"DayOfWeek\" and \"PdDistrict\" includes categorical variables that should be converted to binary data which the machine can understand without preferring one over the other, why Pandas' get_dummies function is used.\n",
    "\n",
    "Because the crime category is what should be predicted, these are not converted to binary data, but are just given a numeric representation using Sklearn's LabelEncoder.\n",
    "\n",
    "The \"Date\" and \"Time\" features are also kind of included. When the raw crime data is loaded into a dataframe, the columns are just treated as strings. We want to use them to determine how time influceses the crimes. To make the machine understand this, however, the columns are merged together to a datetime column \"Date_Time\" where the dates are converted to their ordinal numeric values.\n",
    "At the time this seemed smart, but after doing some thinking, it would probably have been better to split the date times up into something like; year, month, day, hour, minute or something, as humans, and therefor crimes, follow more patterns of our gregorian calendar rather than a UNIX timestamp...\n",
    "\n",
    "**Report accuracy. Discuss the model performance.**\n",
    "\n",
    "Well, the accuracy is around 57% for the Random Forest classifier, an 14 % better accuracy of the baseline of 50/50. This is probably not good enough for any practical application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-lafayette",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv(\"weather_data.csv\")\n",
    "\n",
    "# Format date and time for easy processing and training\n",
    "weather[dt] = pd.to_datetime(weather[\"date\"])\n",
    "weather[year] = weather[dt].dt.year\n",
    "weather[month] = weather[dt].dt.month\n",
    "weather[day] = weather[dt].dt.day\n",
    "weather[hour] = weather[dt].dt.hour\n",
    "\n",
    "weather[hour_of_month] = weather.apply(lambda row: row[dt].day * 24 + row[hour], axis=1)\n",
    "weather[hour_of_week] = weather.apply(lambda row: row[dt].dayofweek * 24 + row[hour], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reverse-value",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's merge the weather and crime dataframes together!\n",
    "\n",
    "merged_df = pd.merge(crime_df, weather, how=\"left\", on=[year, month, day, hour, hour_of_month, hour_of_week])\n",
    "merged_df.dropna()\n",
    "\n",
    "weather_features = [\"weather\", \"humidity\", \"temperature\", \"pressure\", \"wind_speed\"]\n",
    "weather_dummies = [\"weather\"]\n",
    "\n",
    "concatted_features = np.concatenate([weather_features, crime_features])\n",
    "concatted_dummies = np.concatenate([weather_dummies, crime_dummies])\n",
    "\n",
    "Xwc = merged_df[concatted_features].copy()\n",
    "Xwc = Xwc.dropna()\n",
    "\n",
    "# Label encode the categories\n",
    "Xwc[cat] = le.fit_transform(Xwc[cat])\n",
    "\n",
    "# One-hot encode the categorical data\n",
    "Xwc = pd.get_dummies(Xwc, columns=concatted_dummies)\n",
    "\n",
    "# Labels will be the values we want to predict\n",
    "ywc = np.array(Xwc[cat])\n",
    "\n",
    "# We remove the labels from the crime dataframe to get all the values we need for the features\n",
    "Xwc = Xwc.drop(cat, axis=1)\n",
    "\n",
    "# We save the feature names for later\n",
    "Xwc_list = list(Xwc.columns)\n",
    "\n",
    "# Convert the dataframe to a numpy array so we can work with the features\n",
    "Xwc = np.array(Xwc)\n",
    "\n",
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "Xwc_train, Xwc_test, ywc_train, ywc_test = train_test_split(Xwc, ywc, test_size = 0.25, random_state = 42)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=99, random_state=42, max_depth=7)\n",
    "clf.fit(Xwc_train, ywc_train)\n",
    "\n",
    "preds = clf.predict(Xwc_test)\n",
    "print(\"Test\")\n",
    "print('- Mean Absolute Error:', round(mean_absolute_error(ywc_test, preds), 2), 'degrees.')\n",
    "\n",
    "# Weatherd crime accuracy\n",
    "wcacc = clf.score(Xwc_test, ywc_test)\n",
    "print(\"- Accuracy: \", 100 * round(wcacc, 2), \"%\\n\")\n",
    "\n",
    "print(\"Using weather data, the predictions become\", round(((wcacc / cacc) - 1) * 100, 2), \"% better\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-newark",
   "metadata": {},
   "source": [
    "## Part 2B\n",
    "\n",
    "**Report accuracy**\n",
    "\n",
    "Well, the accuracy haven't really improved much.\n",
    "\n",
    "**Discuss how the model performance changes relative to the version with no weather data**\n",
    "\n",
    "Not much, between 3 and 5 percentage points.\n",
    "\n",
    "**Discuss what you have learned about crime from including weather data in your model**\n",
    "\n",
    "That weather is probably not a good indicator for predicting crimes. Especially burglary might not be the best fit, as it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-packing",
   "metadata": {},
   "source": [
    "## Part 3: Data visualization\n",
    "\n",
    "* Create the Bokeh visualization from Part 2 of the Week 8 Lecture, displayed in a beautiful `.gif` below. \n",
    "* Provide nice comments for your code. Don't just use the `# inline comments`, but the full Notebook markdown capabilities and explain what you're doing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-swimming",
   "metadata": {},
   "source": [
    "Initially, we import the relevant libraries and load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-truth",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bokeh.models import ColumnDataSource, FactorRange, Legend\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.transform import factor_cmap\n",
    "from bokeh.palettes import Category20\n",
    "from bokeh.io import output_notebook, push_notebook, show\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import folium\n",
    "import datetime\n",
    "\n",
    "df = pd.read_csv(\"Police_Department_Incident_Reports__Historical_2003_to_May_2018.csv\")\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.loc[df.Date > datetime.datetime(year=2010, month=1, day=1)]\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-expansion",
   "metadata": {},
   "source": [
    "Then, we group by category and hours and pick out a few relevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-action",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Time'] = pd.to_datetime(df['Time'])\n",
    "df[\"Hours\"] = df.Time.dt.strftime('%H')\n",
    "df_hours = df.groupby([\"Category\", \"Hours\"]).agg('count')\n",
    "df_hours = df_hours.reset_index()[[\"Category\", \"Hours\", \"IncidntNum\"]]\n",
    "df_hours.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-opinion",
   "metadata": {},
   "source": [
    "Aftwards, the sum of each crime is calculated and added to a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-arena",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "focuscrimes = set(['WEAPON LAWS', 'PROSTITUTION', 'DRIVING UNDER THE INFLUENCE', 'ROBBERY', 'BURGLARY', 'ASSAULT', 'DRUNKENNESS', 'DRUG/NARCOTIC', 'TRESPASS', 'LARCENY/THEFT', 'VANDALISM', 'VEHICLE THEFT', 'STOLEN PROPERTY', 'DISORDERLY CONDUCT'])\n",
    "crimes = df_hours.Category.unique()\n",
    "\n",
    "for crime in df_hours.Category.unique():\n",
    "    df_hours[crime] = df_hours.loc[df_hours.Category==crime].IncidntNum\n",
    "    \n",
    "result_df = df_hours.groupby(\"Hours\").agg(\"sum\")\n",
    "result_df.drop('IncidntNum', inplace=True, axis=1)\n",
    "result_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-empire",
   "metadata": {},
   "source": [
    "Then, we normalize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-concept",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, data in result_df.iteritems():\n",
    "    result_df[name] = result_df[name]/sum(data)\n",
    "    \n",
    "result_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = ColumnDataSource(result_df)\n",
    "hours = [str(x) for x in range(0,23)]\n",
    "\n",
    "p = figure(x_range = FactorRange(factors=result_df.index), plot_width=1200, plot_height=500)\n",
    "\n",
    "bar = {}\n",
    "for indx,name in enumerate(focuscrimes):\n",
    "    bar[name] = p.vbar(x='Hours',  top=name, source=source, \n",
    "                 legend_label=name, muted_color=\"white\", muted_alpha=0, color=Category20[14][indx], alpha=0.8) \n",
    "    \n",
    "p.legend.click_policy=\"mute\" ### assigns the click policy (you can try to use ''hide'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-procurement",
   "metadata": {},
   "source": [
    "We also need to move the legend to the right of the figure to avoid overlaying the data. First, we remove the existing legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "for indx,name in enumerate(focuscrimes):\n",
    "    items.append((name, [bar[name]]))\n",
    "    \n",
    "p.legend.items = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-tutorial",
   "metadata": {},
   "source": [
    "Then, we add the vbars to a list of items togeter with the name of the crime. Finally, the legend is added to the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-period",
   "metadata": {},
   "outputs": [],
   "source": [
    "legend = Legend(items=items, location=(0,0))\n",
    "legend.click_policy=\"mute\"\n",
    "p.add_layout(legend, 'right')\n",
    "show(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
